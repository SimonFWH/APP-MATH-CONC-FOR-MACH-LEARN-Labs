{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:black;\"> Session 4 - Lab Exercise 7</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can download the original Kyphosis dataset from kaggle at https://www.kaggle.com/abbasit/kyphosis-dataset?select=kyphosis.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 81 entries, 0 to 80\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Kyphosis  81 non-null     object\n",
      " 1   Age       81 non-null     int64 \n",
      " 2   Number    81 non-null     int64 \n",
      " 3   Start     81 non-null     int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "Kyphosis_df = pd.read_csv('kyphosis.csv')\n",
    "Kyphosis_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Kyphosis_df.drop('Kyphosis',axis=1)\n",
    "y = Kyphosis_df['Kyphosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDecisionTreeClassifier(\\n    criterion='gini',\\n    splitter='best',\\n    max_depth=None,\\n    min_samples_split=2,\\n    min_samples_leaf=1,\\n    min_weight_fraction_leaf=0.0,\\n    max_features=None,\\n    random_state=None,\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    class_weight=None,\\n    presort=False,\\n)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the right \"parameters/hyperparameters of ML models can significantly affect their performacnce\".\n",
    "# let's begin exploring those parameters more for a DT model.\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "'''\n",
    "DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    class_weight=None,\n",
    "    presort=False,\n",
    ")\n",
    "'''\n",
    "# Let's consider these three parameters: criterion, splitter, min_samples_split\n",
    "    # for parameters consider two cases 'gini' and 'entropy'\n",
    "    # for splitter consider two cases 'best' and 'random'\n",
    "    # for min_samples_split consider two cases 2 and 5\n",
    "    # explain and dicuss your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier using criterion='gini', splitter='best' and min_samples_split=2, with accuracy_score: 0.84\n",
      "[[20  3]\n",
      " [ 1  1]]\n",
      "DecisionTreeClassifier using criterion='gini', splitter='best' and min_samples_split=5, with accuracy_score: 0.88\n",
      "[[21  2]\n",
      " [ 1  1]]\n",
      "DecisionTreeClassifier using criterion='gini', splitter='random' and min_samples_split=2, with accuracy_score: 0.8\n",
      "[[19  4]\n",
      " [ 1  1]]\n",
      "DecisionTreeClassifier using criterion='gini', splitter='random' and min_samples_split=5, with accuracy_score: 0.88\n",
      "[[21  2]\n",
      " [ 1  1]]\n",
      "DecisionTreeClassifier using criterion='entropy', splitter='best' and min_samples_split=2, with accuracy_score: 0.84\n",
      "[[20  3]\n",
      " [ 1  1]]\n",
      "DecisionTreeClassifier using criterion='entropy', splitter='best' and min_samples_split=5, with accuracy_score: 0.88\n",
      "[[21  2]\n",
      " [ 1  1]]\n",
      "DecisionTreeClassifier using criterion='entropy', splitter='random' and min_samples_split=2, with accuracy_score: 0.96\n",
      "[[23  0]\n",
      " [ 1  1]]\n",
      "DecisionTreeClassifier using criterion='entropy', splitter='random' and min_samples_split=5, with accuracy_score: 0.88\n",
      "[[21  2]\n",
      " [ 1  1]]\n"
     ]
    }
   ],
   "source": [
    "for i in ([\"gini\", \"entropy\"]):\n",
    "    for j in ([\"best\", \"random\"]):\n",
    "        for k in([2, 5]):\n",
    "            dtree_gini_0 = DecisionTreeClassifier(\n",
    "                criterion=i,\n",
    "                splitter=j,\n",
    "                min_samples_split=k\n",
    "                )\n",
    "            dtree_gini_0.fit(X_train,y_train)\n",
    "            DTC_predictions_0 = dtree_gini_0.predict(X_test)\n",
    "\n",
    "            score = accuracy_score(y_test, DTC_predictions_0) \n",
    "            print(f\"DecisionTreeClassifier using criterion='{i}', splitter='{j}' and min_samples_split={k}, with accuracy_score: {score}\")\n",
    "            print(confusion_matrix(y_test,DTC_predictions_0))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "## Best vs random \n",
    "\n",
    "In terms of confusion matrix, the best criterion results in fewer errors and a higher overall accuracy. However, it may be more prone to overfitting and less robust to noisy data. The random criterion results in lower overall accuracy.\n",
    "\n",
    "## Gini vs Entropy\n",
    "\n",
    "In terms of confusion matrix, both Gini criterion and entropy criterion may lead to similar results, but Gini criterion is computationally less expensive and considered to be more robust to noisy data.\n",
    "\n",
    "## Min_Sample_Split\n",
    "\n",
    "\n",
    "When min_samples_split is low, the tree will be grown more, which means it will have more splits and more complex.\n",
    "\n",
    "When min_samples_split is high, the tree will be grown less, which means it will have fewer splits and less complex.\n",
    "\n",
    "In this case, we could observe that when we changed min_sample_split from 2 to 5, itâ€™s giving us better accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8c30bf74a16f0e2c98ae8c70238785d1e68cfff28eb34e74488a221aef73231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
